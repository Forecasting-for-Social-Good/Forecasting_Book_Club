---
title: "ETC3550/ETC5550 Applied&nbsp;forecasting"
author: "Ch5. The forecasters' toolbox"
date: "OTexts.org/fpp3/"
toc: true
colortheme: monashwhite
output:
  binb::monash:
    fig_width: 7
    fig_height: 3.5
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

library(tidyverse)
library(fpp3)
library(lubridate)
source("nicefigs.R")

options(width=60)
```

# A tidy forecasting workflow

## A tidy forecasting workflow

The process of producing forecasts can be split up into a few fundamental steps.

1. Preparing data
2. Data visualisation
3. Specifying a model
4. Model estimation
5. Accuracy \& performance evaluation
6. Producing forecasts

## A tidy forecasting workflow

```{r workflow, echo = FALSE}
line_curve <- function(x, y, xend, yend, ...){
  geom_curve(
    aes(x = x, y = y, xend = xend, yend = yend),
    arrow = arrow(type = "closed", length = unit(0.03, "npc")),
    ...
  )
}

ggplot() +
  geom_text(
    aes(x = x, y = y, label = label),
    data = tribble(
      ~ x, ~ y, ~ label,
      1, 0, "Tidy",
      7/3, 0, "Visualise",
      3, 0.5, "Specify",
      11/3, 0, "Estimate",
      3, -0.5, "Evaluate",
      5, 0, "Forecast"
    ),
    size = 5
  ) +
  geom_segment(
    aes(x = x, y = y, xend = xend, yend = yend),
    data = tribble(
      ~ x, ~ y, ~ xend, ~ yend,
      1.3, 0, 1.9, 0,
      4.1, 0, 4.6, 0
    ),
    arrow = arrow(type = "closed", length = unit(0.03, "npc"))
  ) +
  line_curve(7/3, 0.1, 8/3, 0.5, angle = 250, curvature = -0.3) +
  line_curve(10/3, 0.5, 11/3, 0.1, angle = 250, curvature = -0.3) +
  line_curve(8/3, -0.5, 7/3, -0.1, angle = 250, curvature = -0.3) +
  line_curve(11/3, -0.1, 10/3, -0.5, angle = 250, curvature = -0.3) +
  theme_void() +
  xlim(0.8, 5.2) +
  ylim(-0.6, 0.6) +
  coord_equal(ratio = 1)
```

## Data preparation and visualisation
\fontsize{10}{13}\sf

```{r GDP-plot, fig.height = 3.2}
global_economy %>%
  filter(Country=="Sweden") %>%
  autoplot(GDP) +
    ggtitle("GDP for Sweden") + ylab("$US billions")
```

## Model estimation

The `model()` function trains models to data.

\fontsize{10}{13}\sf

```{r GDP-model, warning=FALSE}
fit <- global_economy %>%
  model(trend_model = TSLM(GDP ~ trend()))
fit
```

A `mable` is a model table, each cell corresponds to a fitted model.

## Producing forecasts

\fontsize{10}{13}\sf

```{r GDP-fc, echo = TRUE, dependson='GDP-model', warning=FALSE}
fit %>% forecast(h = "3 years")
```

A `fable` is a forecast table with point forecasts and distributions.

## Visualising forecasts

\footnotesize

```{r GDP-fc-plot, warning=FALSE, message=FALSE, fig.height=3}
fit %>% forecast(h = "3 years") %>%
  filter(Country=="Sweden") %>%
  autoplot(global_economy) +
    ggtitle("GDP for Sweden") + ylab("$US billions")
```

# Some simple forecasting methods

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `MEAN(y)`: Average method

  * Forecast of all future values is equal to mean of historical data $\{y_1,\dots,y_T\}$.
  * Forecasts: $\hat{y}_{T+h|T} = \bar{y} = (y_1+\dots+y_T)/T$

```{r mean-method-explained, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 3.3}
bricks <- aus_production %>%
  filter(!is.na(Bricks)) %>%
  mutate(average = mean(Bricks))

fc <- bricks %>%
  filter(row_number() == n()) %>% as_tibble() %>%
  unnest(Quarter = list(as.Date(Quarter) + months(c(0, 12*5))))

bricks %>%
  ggplot(aes(x = Quarter, y = Bricks)) +
  geom_line() +
  geom_line(aes(y = average), colour = "blue", linetype = "dashed") +
  geom_line(aes(y = average), data = fc, colour = "blue") +
  ggtitle("Clay brick production in Australia")
```

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `NAIVE(y)`: Naïve method

  * Forecasts equal to last observed value.
  * Forecasts: $\hat{y}_{T+h|T} =y_T$.
  * Consequence of efficient market hypothesis.

```{r naive-method-explained, echo = FALSE, warning = FALSE, fig.height = 3.1}
bricks %>%
  filter(!is.na(Bricks)) %>%
  model(NAIVE(Bricks)) %>%
  forecast(h = "5 years") %>%
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(data = slice(bricks, n()), colour = "blue") +
  ggtitle("Clay brick production in Australia")
```

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `SNAIVE(y ~ lag(m))`: Seasonal naïve method

  * Forecasts equal to last value from same season.
  * Forecasts: $\hat{y}_{T+h|T} =y_{T+h-m(k+1)}$, where $m=$ seasonal period and $k$ is the integer part of $(h-1)/m$.

```{r snaive-method-explained, echo = FALSE, warning = FALSE, fig.height = 2.8}
bricks %>%
  model(SNAIVE(Bricks ~ lag("year"))) %>%
  forecast(h = "5 years") %>%
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(data = slice(bricks, (n()-3):n()), colour = "blue") +
  ggtitle("Clay brick production in Australia")
```

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `RW(y ~ drift())`: Drift method

 * Forecasts equal to last value plus average change.
 * Forecasts:\vspace*{-.7cm}

 \begin{align*}
 \hat{y}_{T+h|T} & =  y_{T} + \frac{h}{T-1}\sum_{t=2}^T (y_t-y_{t-1})\\
                 & = y_T + \frac{h}{T-1}(y_T -y_1).
 \end{align*}\vspace*{-0.2cm}

   * Equivalent to extrapolating a line drawn between first and last observations.

## Some simple forecasting methods

### Drift method

```{r drift-method-explained, echo = FALSE, warning = FALSE}
aus_production %>%
  filter(!is.na(Bricks)) %>%
  model(RW(Bricks ~ drift())) %>%
  forecast(h = "5 years") %>%
  autoplot(aus_production, level = NULL) +
  geom_line(data = slice(aus_production, range(cumsum(!is.na(Bricks)))),
            linetype = "dashed", colour = "blue") +
  ggtitle("Clay brick production in Australia")
```

## Model fitting

The `model()` function trains models to data.

\fontsize{10}{11}\sf

```{r beer-model}
# Set training data from 1992 to 2006
train <- aus_production %>% filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train %>%
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
```

```{r beer-model2, echo=FALSE, dependson='beer-model'}
beer_fit
```

\vspace*{-0.2cm}\begin{alertblock}{}
A \texttt{mable} is a model table, each cell corresponds to a fitted model.
\end{alertblock}

## Producing forecasts

\fontsize{10}{13}\sf

```{r beer-fc, echo = TRUE, dependson='beer-model'}
# Generate forecasts for 14 quarters
beer_fc <- beer_fit %>% forecast(h = 14)
```

```{r beer-fbl, echo = FALSE, dependson='beer-fc'}
print(brick_fc, n = 4)
```

\begin{alertblock}{}
A \texttt{fable} is a forecast table with point forecasts and distributions.
\end{alertblock}

## Visualising forecasts

\footnotesize

```{r beer-fc-plot, warning=FALSE, message=FALSE, fig.height=3, dependson='beer-fc'}
# Plot forecasts against actual values
beer_fc %>%
  autoplot(train, level = NULL) +
  autolayer(filter_index(aus_production, "2007 Q1" ~ .), color = "black") +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour = guide_legend(title = "Forecast"))
```

## Google closing stock price

\fontsize{9}{10}\sf

```{r fbf2, fig.show='hide'}
# Re-index based on trading days
google_stock <- gafa_stock %>%
  filter(Symbol == "GOOG") %>%
  mutate(day = row_number()) %>%
  update_tsibble(index = day, regular = TRUE)
# Filter the year of interest
google_2015 <- google_stock %>% filter(year(Date) == 2015)
# Fit the models
google_fit <- google_2015 %>%
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )
# Produce forecasts for the 19 trading days in January 2015
google_fc <- google_fit %>% forecast(h = 19)
# A better way using a tsibble to determine the forecast horizons
google_jan_2016 <- google_stock %>%
  filter(yearmonth(Date) == yearmonth("2016 Jan"))
google_fc <- google_fit %>% forecast(google_jan_2016)
# Plot the forecasts
google_fc %>%
  autoplot(google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, color = "black") +
  ggtitle("Google stock (daily ending 31 Dec 2015)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour = guide_legend(title = "Forecast"))
```

## Google closing stock price

```{r ref.label='fbf2', echo=FALSE, fig.height=4.6}
```

## Your turn

 * Produce forecasts using an appropriate benchmark method for household wealth (`hh_budget`). Plot the results using `autoplot()`.
 * Produce forecasts using an appropriate benchmark method for Australian takeaway food turnover (`aus_retail`). Plot the results using `autoplot()`.

# Residual diagnostics

## Fitted values

 - $\hat{y}_{t|t-1}$ is the forecast of $y_t$ based on observations $y_1,\dots,y_{t-1}$.
 - We call these "fitted values".
 - Sometimes drop the subscript: $\hat{y}_t \equiv \hat{y}_{t|t-1}$.
 - Often not true forecasts since parameters are estimated on all data.

### For example:

 - $\hat{y}_{t} = \bar{y}$ for average method.
 - $\hat{y}_{t} = y_{t-1} + (y_{T}-y_1)/(T-1)$ for drift method.

## Forecasting residuals

\begin{block}{}
\textbf{Residuals in forecasting:} difference between observed value and its fitted value: $e_t = y_t-\hat{y}_{t|t-1}$.
\end{block}
\pause\fontsize{13}{15}\sf

\alert{Assumptions}

  1. $\{e_t\}$ uncorrelated. If they aren't, then information left in  residuals that should be used in computing forecasts.
  2. $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

\pause

\alert{Useful properties} (for distributions & prediction intervals)

  3. $\{e_t\}$ have constant variance.
  4. $\{e_t\}$ are normally distributed.

## Facebook closing stock price
\fontsize{9}{10}\sf

```{r fbf, fig.height=3.5}
fb_stock <- gafa_stock %>%
  filter(Symbol == "FB") %>%
  mutate(trading_day = row_number()) %>%
  update_tsibble(index = trading_day, regular = TRUE)
fb_stock %>% autoplot(Close)
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r augment}
fit <- fb_stock %>% model(NAIVE(Close))
augment(fit)
```

\only<2>{\begin{textblock}{6}(.5,6.9)\fontsize{14}{16}\sf
\begin{alertblock}{Na\"{\i}ve forecasts:}\vspace*{-0.4cm}
\begin{align*}
\hat{y}_{t|t-1} & = y_{t-1}\\
e_t & = y_t - \hat{y}_{t|t-1} = y_t-y_{t-1}
\end{align*}
\end{alertblock}\end{textblock}}

\only<2>{\begin{textblock}{1}(9,2.7)\fontsize{14}{16}\sf
\begin{alertblock}{}\centerline{$\hat{y}_{t|t-1}$}\end{alertblock}\end{textblock}}
\only<2>{\begin{textblock}{.8}(10.5,2.7)\fontsize{14}{16}\sf
\begin{alertblock}{}\centerline{$\phantom{\hat{y}_{|}}{e}_{t}\phantom{\hat{y}_{|}}$}\end{alertblock}\end{textblock}}

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj4, echo=TRUE, warning=FALSE, fig.height=3.7}
augment(fit) %>%
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj4a, echo=TRUE, warning=FALSE, fig.height=3.7}
augment(fit) %>%
  filter(trading_day > 1100) %>%
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj5, echo=TRUE, warning = FALSE}
augment(fit) %>%
  autoplot(.resid) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naïve method")
```

## Facebook closing stock price
\fontsize{11}{11}\sf

```{r dj6, warning=FALSE}
augment(fit) %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 150) +
  ggtitle("Histogram of residuals")
```

## Facebook closing stock price
\fontsize{11}{11}\sf

```{r dj7}
augment(fit) %>%
  ACF(.resid) %>%
  autoplot() + ggtitle("ACF of residuals")
```

## ACF of residuals

  * We assume that the residuals are white noise (uncorrelated, mean zero, constant variance). If they aren't, then there is information left in  the residuals that should be used in computing forecasts.

  * So a standard residual diagnostic is to check the ACF of the residuals of a forecasting method.

  * We *expect* these to look like white noise.

## Portmanteau tests

Consider a *whole set* of $r_{k}$  values, and develop a test to see whether the set is significantly different from a zero set.\pause

\begin{block}{Box-Pierce test\phantom{g}}
\centerline{$\displaystyle
Q = T \sum_{k=1}^h r_k^2$}
where $h$  is max lag being considered and $T$ is number of observations.
\end{block}

  * If each $r_k$ close to zero, $Q$ will be **small**.
  * If some $r_k$ values large (positive or negative), $Q$ will be **large**.

## Portmanteau tests

Consider a *whole set* of $r_{k}$  values, and develop a test to see whether the set is significantly different from a zero set.

\begin{block}{Ljung-Box test}
\centerline{$\displaystyle
 Q^* = T(T+2) \sum_{k=1}^h (T-k)^{-1}r_k^2$}
where $h$  is max lag being considered and $T$ is number of observations.
\end{block}

  * My preferences: $h=10$ for non-seasonal data, $h=2m$ for seasonal data.
  * Better performance, especially in small samples.

\vspace*{10cm}

## Portmanteau tests
\fontsize{13}{15}\sf

  * If data are WN, $Q^*$ has $\chi^2$ distribution with  $(h - K)$ degrees of freedom where $K=$ no.\ parameters in model.
  * When applied to raw data, set $K=0$.

\fontsize{11}{12}\sf

```{r dj9, echo=TRUE}
augment(fit) %>%
  features(.resid, ljung_box, lag=10, dof=0)
```

## `gg_tsresiduals` function
\fontsize{11}{12}\sf

```{r dj10, echo=TRUE, fig.height=4, warning = FALSE}
gg_tsresiduals(fit)
```

## Your turn

Compute seasonal naïve forecasts for quarterly Australian beer production from 1992.

\fontsize{10}{12}\sf

```{r, results = 'hide', fig.show='hide'}
recent <- aus_production %>% filter(year(Quarter) >= 1992)
fit <- recent %>% model(SNAIVE(Beer))
fit %>% forecast() %>% autoplot(recent)
```

\fontsize{14}{15}\sf

Test if the residuals are white noise.

\fontsize{10}{12}\sf

```{r, results = 'hide', fig.show='hide', warning = FALSE}
augment(fit) %>% features(.resid, ljung_box, lag=10, dof=0)
gg_tsresiduals(fit)
```

\fontsize{14}{15}\sf

What do you conclude?

# Distributional forecasts and prediction intervals

## Forecast distributions

 * A forecast $\hat{y}_{T+h|T}$ is (usually) the mean of the conditional distribution $y_{T+h} \mid y_1, \dots, y_{T}$.
 * Most time series models produce normally distributed forecasts.
 * The forecast distribution describes the probability of observing any future value.

## Forecast distributions

\fontsize{14}{18}\sf

Assuming residuals are normal, uncorrelated, sd = $\hat\sigma$:

\begin{block}{}
\begin{tabular}{ll}
\bf Mean: & $\hat{y}_{T+h|T} \sim N(\bar{y}, (1 + 1/T)\hat{\sigma}^2)$\\[0.2cm]
\bf Naïve: & $\hat{y}_{T+h|T} \sim N(y_T, h\hat{\sigma}^2)$\\[0.2cm]
\bf Seasonal naïve: & $\hat{y}_{T+h|T} \sim N(y_{T+h-m(k+1)}, (k+1)\hat{\sigma}^2)$\\[0.2cm]
\bf Drift: & $\hat{y}_{T+h|T} \sim N(y_T + \frac{h}{T-1}(y_T - y_1),h\frac{T+h}{T}\hat{\sigma}^2)$
\end{tabular}
\end{block}

where $k$ is the integer part of $(h-1)/m$.

Note that when $h=1$ and $T$ is large, these all give the same approximate forecast variance: $\hat{\sigma}^2$.

## Prediction intervals

 * A prediction interval gives a region within which we expect $y_{T+h}$ to lie with a specified probability.
 * Assuming forecast errors are normally distributed, then a 95% PI is
 \begin{alertblock}{}
\centerline{$
  \hat{y}_{T+h|T} \pm 1.96 \hat\sigma_h
$}
\end{alertblock}
where $\hat\sigma_h$ is the st dev of the $h$-step distribution.

 * When $h=1$, $\hat\sigma_h$ can be estimated from the residuals.

## Prediction intervals
\fontsize{10}{12}\sf

```{r brick-fc-interval, dependson='brick-fc'}
brick_fc %>% hilo(level = 95)
```

## Prediction intervals

 * Point forecasts are often useless without a measure of uncertainty (such as prediction intervals).
 * Prediction intervals require a stochastic model (with random errors, etc).
 * Multi-step forecasts for time series require a more sophisticated approach (with PI getting wider as the forecast horizon increases).

## Prediction intervals

  * Computed automatically from the forecast distribution.
  * Use `level` argument to control coverage.
  * Check residual assumptions before believing them (we will see this next class).
  * Usually too narrow due to unaccounted uncertainty.

# Forecasting with transformations

## Modelling with transformations

Transformations used in the left of the formula will be automatically back-transformed. To model log-transformed food retailing turnover, you could use:

\fontsize{13}{15}\sf

```{r food, echo=TRUE}
food <- aus_retail %>%
  filter(Industry == "Food retailing") %>%
  summarise(Turnover = sum(Turnover))
```

```{r food-bt-fit, dependson='food'}
fit <- food %>%
  model(SNAIVE(log(Turnover)))
```

## Forecasting with transformations

```{r food-bt-fc, dependson='food-bt-fit'}
fc <- fit %>%
  forecast(h = "3 years")
```

\fontsize{10}{13}\sf

```{r food-bt-fbl, echo = FALSE, dependson='food-bt-fc'}
print(fc, n = 6)
```

## Forecasting with transformations
\fontsize{12}{13}\sf

```{r elec9,echo=TRUE,fig.height=4, dependson='food-bt-fc'}
fc %>% autoplot(filter(food, year(Month) > 2010))
```

## Bias adjustment

  * Back-transformed point forecasts are medians.
  * Back-transformed PI have the correct coverage.

\pause

**Back-transformed means**

Let $X$ be have mean $\mu$ and variance $\sigma^2$.

Let $f(x)$ be back-transformation function, and $Y=f(X)$.

Taylor series expansion about $\mu$:
$$f(X) = f(\mu) + (X-\mu)f'(\mu) + \frac{1}{2}(X-\mu)^2f''(\mu).$$\pause

\begin{alertblock}{}
\centerline{$\E[Y] = \E[f(X)] = f(\mu) + \frac12 \sigma^2 f''(\mu)$}
\end{alertblock}

## Bias adjustment

\fontsize{13}{15}\sf

**Box-Cox back-transformation:**
\begin{align*}
y_t &= \left\{\begin{array}{ll}
        \exp(w_t)      & \quad \lambda = 0; \\
        (\lambda W_t+1)^{1/\lambda}  & \quad \lambda \ne 0.
\end{array}\right. \\
f(x) &= \begin{cases}
                        e^x & \quad\lambda=0;\\
 (\lambda x + 1)^{1/\lambda} & \quad\lambda\ne0.
 \end{cases}\\
f''(x) &= \begin{cases}
                        e^x & \quad\lambda=0;\\
 (1-\lambda)(\lambda x + 1)^{1/\lambda-2} & \quad\lambda\ne0.
 \end{cases}
\end{align*}\pause
\begin{alertblock}{}
\centerline{$\E[Y] = \begin{cases}
                        e^\mu\left[1+\frac{\sigma^2}{2}\right] & \quad\lambda=0;\\
 (\lambda \mu + 1)^{1/\lambda}\left[1+\frac{\sigma^2(1-\lambda)}{2(\lambda\mu+1)^2}\right] & \quad\lambda\ne0.
 \end{cases}$}
\end{alertblock}

## Bias adjustment
\fontsize{9}{9}\sf

```{r biasadj, fig.height=3.2, message=FALSE}
eggs <- as_tsibble(fma::eggs)
fit <- eggs %>% model(RW(log(value) ~ drift()))
fc <- fit %>% forecast(h=50)
fc_biased <- fit %>% forecast(h=50, bias_adjust = FALSE)
eggs %>% autoplot(value) + xlab("Year") +
  autolayer(fc_biased, level = 80) +
  autolayer(fc, colour = "red", level = NULL)
```

# Forecasting and decomposition

## Forecasting and decomposition

  *  Forecast seasonal component by repeating the last year
  *  Forecast seasonally adjusted data using non-seasonal time series method.
  *  Combine forecasts of seasonal component with forecasts of seasonally adjusted data to get forecasts of original data.
  *  Sometimes a decomposition is useful just for understanding the data before building a separate forecasting model.

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade") %>%
  select(-Series_ID)
us_retail_employment
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail1, echo=TRUE, fig.height=3.2}
dcmp <- us_retail_employment %>%
  model(STL(Employed)) %>%
  components() %>% select(-.model)
dcmp
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail2, echo=TRUE, fig.height=3.2}
dcmp %>%
  model(NAIVE(season_adjust)) %>%
  forecast() %>%
  autoplot(dcmp) +
  ggtitle("Naive forecasts of seasonally adjusted data")
```

\vspace*{10cm}

## US Retail Employment
\fontsize{10}{11}\sf

```{r usretail3, echo=TRUE, fig.height=2.8}
us_retail_employment %>%
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window = 7), robust = TRUE),
    NAIVE(season_adjust)
  )) %>%
  forecast() %>%
  autoplot(us_retail_employment)
```

\vspace*{10cm}

## Decomposition models

`decomposition_model()` creates a decomposition model

 * You must provide a method for forecasting the `season_adjust` series.
 * A seasonal naive method is used by default for the `seasonal` components.
 * The variances from both the seasonally adjusted and seasonal forecasts are combined.

# Evaluating forecast accuracy

## Training and test sets

```{r traintest, fig.height=1, echo=FALSE, cache=TRUE}
train = 1:18
test = 19:24
par(mar=c(0,0,0,0))
plot(0,0,xlim=c(0,26),ylim=c(0,2),xaxt="n",yaxt="n",bty="n",xlab="",ylab="",type="n")
arrows(0,0.5,25,0.5,0.05)
points(train, train*0+0.5, pch=19, col="blue")
points(test,  test*0+0.5,  pch=19, col="red")
text(26,0.5,"time")
text(10,1,"Training data",col="blue")
text(21,1,"Test data",col="red")
```

-   A model which fits the training data well will not necessarily forecast well.
-   A perfect fit can always be obtained by using a model with enough parameters.
-   Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.
  * The test set must not be used for *any* aspect of model development or calculation of forecasts.
  * Forecast accuracy is based only on the test set.

## Forecast errors

Forecast "error": the difference between an observed value and its forecast.
$$
  e_{T+h} = y_{T+h} - \hat{y}_{T+h|T},
$$
where the training data is given by $\{y_1,\dots,y_T\}$

- Unlike residuals, forecast errors on the test set involve multi-step forecasts.
- These are *true* forecast errors as the test data is not used in computing $\hat{y}_{T+h|T}$.

## Measures of forecast accuracy

```{r beer-fc-1, echo=FALSE, fig.height=4}
train <- aus_production %>%
  filter(between(year(Quarter), 1992, 2007))
beer <- aus_production %>%
  filter(year(Quarter) >= 1992)
beer_fc_plot <- train %>%
  model(
    Mean = MEAN(Beer),
    Naive = NAIVE(Beer),
    Seasonal_naive = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  ) %>%
  forecast(h=11) %>%
  autoplot(beer, level = NULL) +
    ggtitle("Forecasts for quarterly beer production") +
    xlab("Year") + ylab("Megalitres") +
    guides(colour=guide_legend(title="Forecast"))
beer_fc_plot
```

## Measures of forecast accuracy

\begin{tabular}{rl}
$y_{T+h}=$ & $(T+h)$th observation, $h=1,\dots,H$ \\
$\pred{y}{T+h}{T}=$ & its forecast based on data up to time $T$. \\
$e_{T+h} =$  & $y_{T+h} - \pred{y}{T+h}{T}$
\end{tabular}

\begin{align*}
\text{MAE} &= \text{mean}(|e_{T+h}|) \\[-0.2cm]
\text{MSE} &= \text{mean}(e_{T+h}^2) \qquad
&&\text{RMSE} &= \sqrt{\text{mean}(e_{T+h}^2)} \\[-0.1cm]
\text{MAPE} &= 100\text{mean}(|e_{T+h}|/ |y_{T+h}|)
\end{align*}\pause

  * MAE, MSE, RMSE are all scale dependent.
  * MAPE is scale independent but is only sensible if $y_t\gg 0$ for all $t$, and $y$ has a natural zero.

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For non-seasonal time series,
$$
  Q = (T-1)^{-1}\sum_{t=2}^T |y_t-y_{t-1}|
$$
works well. Then MASE is equivalent to MAE relative to a naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For seasonal time series,
$$
  Q = (T-m)^{-1}\sum_{t=m+1}^T |y_t-y_{t-m}|
$$
works well. Then MASE is equivalent to MAE relative to a seasonal naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

```{r beer-fc-2, echo=FALSE, fig.height=4}
beer_fc_plot
```

## Measures of forecast accuracy

\fontsize{12}{14}\sf

```{r beer-forecasts, results='hide'}
recent_production <- aus_production %>%
  filter(year(Quarter) >= 1992)
train <- recent_production %>%
  filter(year(Quarter) <= 2007)
beer_fit <- train %>%
  model(
    Mean = MEAN(Beer),
    Naive = NAIVE(Beer),
    Seasonal_naive = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  )
beer_fc <- beer_fit %>%
  forecast(h = 10)
```

## Measures of forecast accuracy
\fontsize{11}{11}\sf

```{r beer-train-accuracy, eval=FALSE}
accuracy(beer_fit)
```

\fontsize{9}{9}\sf

```{r beer-train-table, echo=FALSE}
accuracy(beer_fit) %>%
  arrange(.model) %>%
  select(.model, .type, RMSE, MAE, MAPE, MASE)
```

\fontsize{11}{11}\sf

```{r beer-test-accuracy, eval=FALSE}
accuracy(beer_fc, recent_production)
```

\fontsize{9}{9}\sf

```{r beer-test-table, echo=FALSE}
accuracy(beer_fc, recent_production) %>%
  arrange(.model) %>%
  select(.model, .type, RMSE, MAE, MAPE, MASE)
```

\vspace*{10cm}

## Poll: true or false?

  1. Good forecast methods should have normally distributed residuals.
  2. A model with small residuals will give good forecasts.
  3. The best measure of forecast accuracy is MAPE.
  4. If your model doesn't forecast well, you should make it more complicated.
  5. Always choose the model with the best forecast accuracy as measured on the test set.

# Time series cross-validation

## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest2, fig.height=1, echo=FALSE, cache=TRUE}
train = 1:18
test = 19:24
par(mar=c(0,0,0,0))
plot(0,0,xlim=c(0,26),ylim=c(0,2),xaxt="n",yaxt="n",bty="n",xlab="",ylab="",type="n")
arrows(0,0.5,25,0.5,0.05)
points(train, train*0+0.5, pch=19, col="blue")
points(test,  test*0+0.5,  pch=19, col="red")
text(26,0.5,"time")
text(10,1,"Training data",col="blue")
text(21,1,"Test data",col="red")
```

\vspace*{10cm}

## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest3, fig.height=1, echo=FALSE, cache=TRUE}
train = 1:18
test = 19:24
par(mar=c(0,0,0,0))
plot(0,0,xlim=c(0,26),ylim=c(0,2),xaxt="n",yaxt="n",bty="n",xlab="",ylab="",type="n")
arrows(0,0.5,25,0.5,0.05)
points(train, train*0+0.5, pch=19, col="blue")
points(test,  test*0+0.5,  pch=19, col="red")
text(26,0.5,"time")
text(10,1,"Training data",col="blue")
text(21,1,"Test data",col="red")
```

**Time series cross-validation**

```{r cv1, cache=TRUE, echo=FALSE, fig.height=4}
par(mar=c(0,0,0,0))
plot(0,0,xlim=c(0,28),ylim=c(0,1),
       xaxt="n",yaxt="n",bty="n",xlab="",ylab="",type="n")
i <- 1
for(j in 1:10)
{
  test <- (16+j):26
  train <- 1:(15+j)
  arrows(0,1-j/20,27,1-j/20,0.05)
  points(train,rep(1-j/20,length(train)),pch=19,col="blue")
  if(length(test) >= i)
    points(test[i], 1-j/20, pch=19, col="red")
  if(length(test) >= i)
    points(test[-i], rep(1-j/20,length(test)-1), pch=19, col="gray")
  else
    points(test, rep(1-j/20,length(test)), pch=19, col="gray")
}
text(28,.95,"time")
```

\pause

 * Forecast accuracy averaged over test sets.
 * Also known as "evaluation on a rolling forecasting origin"

 \vspace*{10cm}

## Creating the rolling training sets {-}

\fontsize{13}{14}\sf

There are three main rolling types which can be used.

* Stretch: extends a growing length window with new data.
* Slide: shifts a fixed length window through the data.
* Tile: moves a fixed length window without overlap.

Three functions to roll a tsibble: `stretch_tsibble()`, `slide_tsibble()`,
and `tile_tsibble()`.

For time series cross-validation, stretching windows are most commonly used.

## Creating the rolling training sets {-}

```{r animate, echo = FALSE, warning = FALSE, message = FALSE, fig.show='animate', interval=1/10, fig.height=4, fig.width=8, aniopts='controls,buttonsize=0.3cm,width=11.5cm'}
library(tidyverse)
library(gganimate)
tourism_melb <- tourism %>%
  filter(Region == "Melbourne", Purpose == "Holiday")
types <- forcats::fct_inorder(c("Slide", "Tile", "Stretch"))
slide_window <- slider(tourism_melb$Quarter, .size = 4) %>%
  map_dfr(function(x) tibble(xmin = min(x), xmax = max(x))) %>%
  mutate(ymin = -Inf, ymax = Inf, group = row_number(), type = types[1])

tile_window <- tiler(tourism_melb$Quarter, .size = 4) %>%
  map_dfr(function(x) tibble(xmin = min(x), xmax = max(x))) %>%
  mutate(ymin = -Inf, ymax = Inf, type = types[2])
tile_window <- tile_window[c(rep(1:20, each = 4)), ] %>%
  mutate(group = row_number())

stretch_window <- stretcher(tourism_melb$Quarter, .init = 4) %>%
  map_dfr(function(x) tibble(xmin = min(x), xmax = max(x))) %>%
  mutate(ymin = -Inf, ymax = Inf, group = row_number(), type = types[3])

window <- bind_rows(slide_window, tile_window, stretch_window)

ggplot() +
  geom_line(aes(x = Quarter, y = Trips), data = tourism_melb, colour = "grey", size = 1.2) +
  geom_rect(aes(
    xmin = xmin, xmax = xmax,
    ymin = ymin, ymax = ymax,
    group = group
  ), data = window,
  fill = "#9ecae1", colour = "#9ecae1", size = 1.5, alpha = 0.6) +
  xlab("Quarter") +
  ylab("Trips") +
  facet_wrap(~ type, ncol = 1) +
  theme_bw() +
  transition_manual(group)
```

## Time series cross-validation {-}

\fontsize{12}{13}\sf

Stretch with a minimum length of 3, growing by 1 each step.

```{r google-stretch, cache=TRUE}
fb_stretch <- fb_stock %>%
  stretch_tsibble(.init = 3, .step = 1) %>%
  filter(.id != max(.id))
```
\fontsize{10}{11}\sf
```{r google-stretch-print, echo = FALSE}
options(width = 80)
fb_stretch %>% select(Date, Close, trading_day, .id) %>% print(n=7)
```

## Time series cross-validation {-}

\small

Estimate RW w/ drift models for each window.

```{r google-fit, cache = TRUE}
fit_cv <- fb_stretch %>%
  model(RW(Close ~ drift()))
```

\fontsize{10}{11}\sf
```{r google-fit-print, echo = FALSE}
print(fit_cv, n = 4)
```

## Time series cross-validation {-}

\small

Produce one step ahead forecasts from all models.

```{r google-fc, cache = TRUE}
fc_cv <- fit_cv %>%
  forecast(h=1)
```

\fontsize{10}{11}\sf
```{r google-fc-print, echo = FALSE}
fc_cv %>% select(-.model) %>% print(n = 4)
```

## Time series cross-validation {-}

\fontsize{11}{11}\sf

```{r google-accuracy, cache = TRUE, results = 'hide', eval = FALSE}
# Cross-validated
fc_cv %>% accuracy(fb_stock)
# Training set
fb_stock %>% model(RW(Close ~ drift())) %>% accuracy()
```

\fontsize{13}{15}\sf

```{r, echo = FALSE, warning = FALSE}
fc_cv %>% accuracy(fb_stock) %>%
  mutate(.type = "Cross-validation") %>%
  bind_rows(
    fb_stock %>%
      model(RW(Close ~ drift())) %>%
      accuracy()
  ) %>%
  transmute(Type = .type, RMSE, MAE, MAPE) %>%
  gt::gt("Type") %>%
  gt::fmt_number(columns=2:4, decimals=3) %>%
  gt::as_latex()
```

A good way to choose the best forecasting model is to find the model with the smallest RMSE computed using time series cross-validation.
